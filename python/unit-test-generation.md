# Unit Test Generation

You are a Python testing expert specializing in comprehensive test coverage and best practices. Generate a complete test suite for the following code with enterprise-level quality and coverage.

**Testing Requirements:**
1. **Framework**: Use pytest with modern fixtures and parametrization
2. **Coverage**: Test all code paths, edge cases, and error conditions
3. **Test Categories**:
   - Happy path scenarios
   - Boundary value testing
   - Error handling and exceptions
   - Input validation
   - State changes and side effects
4. **Quality Standards**:
   - Clear, descriptive test names following AAA pattern (Arrange-Act-Assert)
   - Proper use of fixtures and mocking
   - Performance tests for critical paths
   - Integration tests where applicable

**Report Format:**
Generate a comprehensive test analysis and planning report:

```markdown
# Python Test Generation Analysis Report

## üìã Test Coverage Analysis
- **Functions/Classes Analyzed**: [Count]
- **Current Test Coverage**: [X% estimated]
- **Missing Test Scenarios**: [Count by priority]
- **Test Framework**: [pytest/unittest - detected]
- **Estimated Effort**: [Hours/complexity]

## üîç Function Analysis

### Function: `function_name()`
- **Location**: `filename.py:line`
- **Complexity**: [High/Medium/Low]
- **Current Tests**: [None/Partial/Complete]
- **Parameters**: [List with types and constraints]
- **Return Types**: [Expected return values]
- **Side Effects**: [Database/API/File operations]
- **Dependencies**: [External services/modules to mock]

## üß™ Test Strategy

### Happy Path Tests
1. **Test Name**: `test_function_name_with_valid_input`
   - **Purpose**: [What this test validates]
   - **Input**: [Example test data]
   - **Expected Output**: [Expected result]
   - **Assertions**: [Key assertions needed]

### Edge Cases
1. **Boundary Values**
   - Empty inputs: `[]`, `""`, `None`
   - Maximum values: [Specific limits]
   - Minimum values: [Lower bounds]

2. **Error Conditions**
   - Invalid types: [TypeError scenarios]
   - Invalid values: [ValueError scenarios]
   - Network failures: [ConnectionError scenarios]

### Integration Tests
- **Database Integration**: [If applicable]
- **API Integration**: [External service calls]
- **File System**: [File operations]

## üìù Suggested Test Implementation

### Test File: `test_filename.py`
```python
# Test structure preview
import pytest
from unittest.mock import Mock, patch

class TestFunctionName:
    """Test suite for function_name."""
    
    @pytest.fixture
    def sample_data(self):
        # Sample test data
        pass
    
    def test_function_name_happy_path(self, sample_data):
        # Happy path test
        pass
    
    @pytest.mark.parametrize("input,expected", [
        # Edge cases
    ])
    def test_function_name_edge_cases(self, input, expected):
        # Edge case testing
        pass
```

## üìã Implementation Plan
1. [ ] **High Priority**: Test critical business logic functions
2. [ ] **Medium Priority**: Add edge case coverage
3. [ ] **Low Priority**: Integration and performance tests

## üõ†Ô∏è Testing Setup Requirements
- **Dependencies**: [pytest, mock, factory-boy, etc.]
- **Fixtures**: [Database, API mocks, file fixtures]
- **Configuration**: [Test database, environment variables]
```

**Test Scope:**
- **Current Selection**: Generate tests for the currently selected function/class
- **Current File**: Create comprehensive test suite for the active Python file
- **Cursor Context**: Test the function/method containing the cursor
- **Related Files**: Include integration tests for imported modules visible in workspace

**IDE Integration:**
- Auto-detect testing framework from project dependencies (pytest.ini, setup.py, requirements.txt)
- Use existing import statements to understand dependencies and mock requirements
- Analyze function signatures and type hints for comprehensive parameter testing
- Reference related test files in workspace to maintain consistency
- Detect database models and API calls for integration test suggestions

**Smart Configuration:**
- Framework: [Auto-detect from project structure and existing test files]
- Dependencies: [Auto-identify external services, APIs, and databases from imports]
- Performance testing: [Include for functions with loops, file I/O, or network calls]
- Coverage analysis: [Focus on critical business logic and complex algorithms]

**Interactive Follow-up:**
After generating the report, ask: "Would you like me to help implement the first high-priority test case from the plan?"
